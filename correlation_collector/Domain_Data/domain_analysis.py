#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Functionï¼š
1. Read the domains_all_enriched.jsonl generated by enrich_domain_v2.py
2. Organize into a DataFrame, generating:
  - DMS / verdict / TLD / age_days / entropy / url_count
  - VT: vt_malicious / vt_suspicious
  - AbuseIPDB: abuse_max_conf
  - OTX: otx_pulse_count
3. Perform:
  - Simple time series analysis (requires first_seen / collected_at; will be empty if not present)
  - KMeans clustering (using the above feature)
  - Output:
  - timeseries.csv
  - clusters.csv
  - domain_report.md
4. No OpenCTI import performed (pure analysis version)
"""

import json
import os
from typing import List
from collections import Counter

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

class Config:
    INPUT_ENRICHED = "your_path"
    
    TIMESERIES_CSV = "your_path"
    CLUSTERS_CSV   = "your_path"
    REPORT_MD      = "your_path"

def load_enriched(path: str) -> List[dict]:
    data: List[dict] = []
    if not os.path.exists(path):
        return data
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            data.append(json.loads(line))
    return data

def build_timeseries(df: pd.DataFrame) -> pd.DataFrame:
 
    dates = []
    verdicts = []

    for _, row in df.iterrows():
        rec = row["raw"]
        date_str = rec.get("first_seen") or rec.get("collected_at")
        if not date_str:
            continue
        try:
            d = date_str[:10]  
        except Exception:
            continue
        dates.append(d)
        verdicts.append(rec.get("verdict", "unknown"))

    ts_df = pd.DataFrame({"date": dates, "verdict": verdicts})
    if ts_df.empty:
        return ts_df

    agg = ts_df.groupby(["date", "verdict"]).size().unstack(fill_value=0)
    agg.to_csv(Config.TIMESERIES_CSV)
    return agg

def build_clusters(df: pd.DataFrame, k: int = 4) -> pd.DataFrame:
 
    feat_cols = [
        "dms_score",
        "age_days",
        "entropy",
        "url_count",
        "vt_malicious",
        "vt_suspicious",
        "abuse_max_conf",
        "otx_pulse_count",
    ]

    feat_data = df[feat_cols].fillna(0.0)
    scaler = StandardScaler()
    X = scaler.fit_transform(feat_data)

    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    df["cluster"] = km.fit_predict(X)

    out = df[["domain", "cluster"] + feat_cols]
    out.to_csv(Config.CLUSTERS_CSV, index=False)
    return out


def write_report(df: pd.DataFrame, ts_df: pd.DataFrame):
    total = len(df)
    verdict_cnt = Counter(df["verdict"].fillna("unknown"))

    tlds = df["tld"].fillna("").tolist()
    tld_cnt = Counter(tlds)

    vt_covered = (df["vt_malicious"] + df["vt_suspicious"] > 0).sum()
    abuse_covered = (df["abuse_max_conf"] > 0).sum()
    otx_covered = (df["otx_pulse_count"] > 0).sum()

    with open(Config.REPORT_MD, "w", encoding="utf-8") as f:
        f.write("# Domain Risk Report (VT + AbuseIPDB + OTX)\n\n")
        f.write(f"- Total domains: **{total}**\n")
        f.write(f"- Verdict counts (from local DMS): {dict(verdict_cnt)}\n\n")

        f.write("## Top 10 TLDs\n\n")
        for tld, c in tld_cnt.most_common(10):
            if not tld:
                tld = "(none)"
            f.write(f"- `{tld}`: {c}\n")
        f.write("\n")

        f.write("## External Intelligence Coverage\n\n")
        f.write(f"- VirusTotal: {vt_covered} domains with non-zero stats\n")
        f.write(f"- AbuseIPDB: {abuse_covered} domains with abuse confidence > 0\n")
        f.write(f"- AlienVault OTX: {otx_covered} domains with pulse_count > 0\n\n")

        if not ts_df.empty:
            f.write("## Time Series (see timeseries.csv)\n\n")
            f.write(f"- Dates: {len(ts_df)} rows\n")

        f.write("\n## Clustering\n\n")
        f.write(f"- See `{os.path.basename(Config.CLUSTERS_CSV)}` for cluster labels.\n")

def main():
    records = load_enriched(Config.INPUT_ENRICHED)
    if not records:
        print("No enriched data found.")
        return

    rows = []
    for rec in records:
        feat = rec.get("features") or {}

        vt = rec.get("vt") or {}
        vt_stats = vt.get("last_analysis_stats") or {}

        abuse = rec.get("abuseipdb") or {}
        otx = rec.get("otx") or {}

        rows.append({
            "domain": rec.get("domain"),
            "verdict": rec.get("verdict", "unknown"),
            "dms_score": rec.get("dms_score", 0),
            "tld": feat.get("tld"),
            "age_days": feat.get("age_days"),
            "entropy": feat.get("entropy"),
            "url_count": feat.get("url_count", 0),
            "vt_malicious": vt_stats.get("malicious", 0),
            "vt_suspicious": vt_stats.get("suspicious", 0),
            "abuse_max_conf": abuse.get("max_abuse_confidence", 0),
            "otx_pulse_count": otx.get("pulse_count", 0),
            "raw": rec,
        })

    df = pd.DataFrame(rows)

    ts_df = build_timeseries(df)

    build_clusters(df, k=4)

    write_report(df, ts_df)

    print("Analysis & report done.")
    print(f"- Report: {Config.REPORT_MD}")
    print(f"- Clusters: {Config.CLUSTERS_CSV}")
    if not ts_df.empty:
        print(f"- Timeseries: {Config.TIMESERIES_CSV}")

if __name__ == "__main__":
    main()
